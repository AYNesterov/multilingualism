{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import Counter # descending sort\n",
    "import gspread # Google Sheets\n",
    "from oauth2client.service_account import ServiceAccountCredentials # Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Sheets authorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('#', scope) # path to the file with a key\n",
    "client = gspread.authorize(creds)\n",
    "sheet = client.open('#').get_worksheet(0) # the name of the worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the text corpus from every social group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.vk.com/method/wall.get?\"\n",
    "query = {'access_token':'#', # VK API access_token\n",
    "        'v':'5.103',\n",
    "        'domain':'', # a group's domain\n",
    "        'offset':0,\n",
    "        'extended':0,\n",
    "        'count':1}\n",
    "\n",
    "r_first = requests.get(url,params=query)\n",
    "q_first = r_first.json()\n",
    "\n",
    "post_found = q_first['response']['count'] # total count of posts in a group\n",
    "\n",
    "loops = post_found // 100 + 1 # offset\n",
    "\n",
    "raw_posts = [] # a data set with all the texts\n",
    "\n",
    "#print('found:',post_found)\n",
    "\n",
    "for i in range(loops): # collect every 100 posts\n",
    "    \n",
    "    offset = i*100\n",
    "    second_query = {'access_token':'', # VK API access_token\n",
    "                    'v':'5.103',\n",
    "                    'domain':'#', # a group's domain\n",
    "                    'offset':offset,\n",
    "                    'extended':0,\n",
    "                    'count':100}\n",
    "\n",
    "    r_second = requests.get(url,params=second_query)\n",
    "    q_second = r_second.json()\n",
    "    \n",
    "    post_range = len(q_second['response']['items'])\n",
    "    \n",
    "    for k in range(0,post_range):\n",
    "        raw_item = {'from_id':q_second['response']['items'][k]['from_id'],'post_id':q_second['response']['items'][k]['id'],'date':q_second['response']['items'][k]['date'],'text':q_second['response']['items'][k]['text']} \n",
    "        raw_posts.append(raw_item)\n",
    "        \n",
    "# check if all the posts are collected\n",
    "\n",
    "# print('Posts in total:', len(raw_posts), 'should be:', post_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding tokens in every text and transferring data to Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: the list 'plust_dict' contained all the stems and their regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0 # a raw in Google Sheets\n",
    "\n",
    "for d in plus_dict: # check every regular expresion\n",
    "    \n",
    "    results = [] # a list with the found tokens\n",
    "    token_search = d['regex'] # a single regEX\n",
    "\n",
    "    for t in raw_posts: # checking the text corpus\n",
    "        \n",
    "        tokens_found = re.findall(token_search, t['text'])\n",
    "\n",
    "        if len(tokens_found) != 0: # if found, save it in results\n",
    "            dict_item = {'from_id':t['from_id'],'post_id':t['post_id'],'date':t['date'],'text':t['text'],'tokens':tokens_found} \n",
    "            results.append(dict_item)\n",
    "\n",
    "    invariants = [] # a list with all the word forms\n",
    "    \n",
    "    for l in results:\n",
    "        for s in l['tokens']:\n",
    "            invariants.append(s.lower()) # lowercase\n",
    "    report = Counter(invariants).most_common() # descending sort\n",
    "\n",
    "    results.append({'words':len(invariants),'n_invariants':len(report), 'invariants':dict(report)})\n",
    "\n",
    "    if len(results) > 1:\n",
    "        with open(f\"#/{d['word']}.json\", 'w', encoding='utf-8') as outfile: # save data as a json file\n",
    "            json.dump(results, outfile, ensure_ascii=False)\n",
    "\n",
    "        row = [d['word'],results[-1]['words'],results[-1]['n_invariants'],\n",
    "               f\"{results[-1]['invariants']}\".strip(\"{}\"),\n",
    "               len(results)-1,len(raw_posts),\"#\",\"#\",d['regex']] # add a raw in Google Sheets\n",
    "\n",
    "        sheet.insert_row(row, index)\n",
    "        \n",
    "        # print(index, len(results)-1)\n",
    "        \n",
    "        index = index + 1 # go to the next row\n",
    "\n",
    "    else:\n",
    "        row = [d['word'],0,0,'none',0,len(raw_posts),\"#\",\"#\",d['regex']] # if a stem is not found, add an emty row\n",
    "        sheet.insert_row(row, index)\n",
    "        \n",
    "       # print(index, len(results)-1)\n",
    "    \n",
    "        index = index + 1 # go to the next row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
